Problem Statement 1- Using Apache Spark and Python, Read and Preprocess rows to insure further obtimal structure and Performance
					 for Further Preprocessing.
					 
					 
					 
Problem Statement 2-                            Using Apache Spark and Python, Read and processed data set From step1 and ;
						1-Extract only Recipes that have Beef as one of the ingredients 
						2-Calculate Average cooking time duration per Difficulty level 
						
# Installation						
						
						PySpark Version-"3.0.0"(set the path in enviroments)
						python version 3.8
                                            jdk 8(set the path in enviroments)
	                                        Pycharm
						
# Libraries						
						Pandas
	                                        Numpy
	                                        matplotlib
# Process


	-Extrated Data from PySpark and Save it to local System
	 as Data1.csv
	-Load data in Jupyter NoteBook
	-Perform EDA(Exploratory Data Analysis)
	-Extract the Beef cantain in Recipes
	-calculated Total Cooking Time
	-Calculated Average Cooking Time
	-Labled the Recipes according to Difficulty Level
     -output in report.csv
![Annotation 2020-09-03 115452](https://user-images.githubusercontent.com/21276619/92079750-117abb00-edde-11ea-917e-3d7a47002624.png)
